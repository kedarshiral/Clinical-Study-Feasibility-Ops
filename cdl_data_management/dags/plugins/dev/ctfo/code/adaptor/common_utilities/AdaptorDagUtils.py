"""This module fetches batch status."""
# !/usr/bin/python3
# -*- coding: utf-8 -*-

import json
import logging
import os
import sys

service_directory_path = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(1, service_directory_path)
code_dir_path = os.path.abspath(os.path.join(service_directory_path, "../"))
sys.path.insert(1, code_dir_path)

logger = logging.getLogger('__name__')
hndlr = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
hndlr.setFormatter(formatter)


from MySQLConnectionManager import MySQLConnectionManager
import CommonConstants
import SystemManagerUtility as SystemManagerUtility



def call_OrganizationDataPrep():
    """This method calls the  call_OrganizationDataPrep  code"""
    """This method calls call_OrganizationDataPrep"""
    APPLICATION_CONFIG_FILE = "application_config.json"
    print("os.getcwd()", os.getcwd())
    application_conf = CommonConstants.AIRFLOW_CODE_PATH + "/adaptor/common_utilities/" + APPLICATION_CONFIG_FILE
    configuration = json.load(open(application_conf))
    audit_db = configuration["adapter_details"]["generic_config"]["mysql_db"]
    print("audit_db :",audit_db)
    query_str = (
        "select cluster_id from {audit_db}.{log_data_acquisition_smry_table} "
        " where source_name = 'organization' and payload_id ='5' and adapter_id=5 and status='Running' order by start_time desc limit 1"
    )
    output_query = query_str.format(audit_db=audit_db,
                                    log_data_acquisition_smry_table=CommonConstants.LOG_DATA_ACQUISITION_SMRY_TABLE
                                    )
    logging.info("Query to select cluster id from the log table: %s", str(output_query))
    response_clusterid = MySQLConnectionManager().execute_query_mysql(output_query, False)

    # DJ
    cluster_id_response = response_clusterid[0]["cluster_id"]
    logging.info("Query output cluster id: %s", str(cluster_id_response))

    cluster_id = cluster_id_response
    logger.info("================Executing job in Spark client mode================")
    conf_value="  "
    logger.info("cd /usr/local/airflow/dags/plugins/dev/ctfo/code/adaptor/organizationtrials " + "; sh OrganizationDataPrepAdapter.sh "
                + str(conf_value))

    command_to_execute = "cd /usr/local/airflow/dags/plugins/dev/ctfo/code/adaptor/organizationtrials; " \
                         + " sh OrganizationDataPrepAdapter.sh " \
                         + '"' + str(conf_value) + '"'
    print("command_to_execute ",command_to_execute)
    output = SystemManagerUtility().execute_command(cluster_id, command_to_execute)
    logging.info("Output generated by OrganizationDataPrepAdapter task - " + str(output))

